from collections import defaultdict, deque
from typing import Sequence

import numpy as np
import openai
from pydantic import validate_call
from sentence_transformers import SentenceTransformer

from wrench.grouper import BaseGrouper
from wrench.models import Device, Group
from wrench.utils.prompt_manager import PromptManager

from .config import LLMConfig
from .text_preprocessor import build_cooccurence_network, get_keywords
from .topic_generator import RootTopics, Topic, TopicGenerator

_DOC_PROMPT = PromptManager.get_prompt("embed_documents.txt")
_TOPIC_PROMPT = PromptManager.get_prompt("embed_topics.txt")


class DecisionTree(BaseGrouper):
    """
    Autogenerate named and described clusters.

    Generates hierarchical clusters, seeded by the given seed topics. The cluster
    names are generated by the specified LLM. The document is classified to the
    cluster which has the highest cosine similarity with its embeddings.

    Attributes:
        embedder (SentenceTransformer): The SentenceTransformer model to be used for
            the classification into clusters.
        client (openai.OpenAI): The OpenAI client for generating hierarchical
            cluster schema.
        llm_model (str): The LLM model used in conjunction with the OpenAI client.
        topic_embeddings (dict[str, np.ndarray]): Generated embeddings for the
            topics
    """

    @validate_call
    def __init__(
        self,
        llm_config: LLMConfig,
        embeddings_model: str = "intfloat/multilingual-e5-large-instruct",
    ):
        """
        Initialize the DecisionTree Grouper.

        Arguments:
            llm_config (LLMConfig): The LLM configuration including host, model
                and api_key. By default this is set to use an "ollama" as the API key.
                To use OpenAI's models, generate an API key on the OpenAI Platform.
            embeddings_model (str): The embeddings model compatible with the
                `SentenceTransformers` library. Defaults to
                `intfloat/multilingual-e5-large-instruct`, use `all-MiniLM-L12-v2` for
                english data.
        """
        super().__init__()

        self.embedder = SentenceTransformer(embeddings_model)
        self.llm_model = llm_config.model
        self.client = openai.OpenAI(
            base_url=llm_config.base_url, api_key=llm_config.api_key
        )

        self.topic_embeddings: dict[str, np.ndarray] = {}

    def load_documents(self, documents: list[str]) -> list[np.ndarray]:
        # cleaned_docs = preprocess(documents)
        self.logger.info("Embedding documents")
        return [
            self.embedder.encode(
                doc,
                prompt=_DOC_PROMPT,
                convert_to_numpy=True,
            )
            for doc in documents
        ]

    def get_topics(self, docs: Sequence[str]):
        keywords = get_keywords("keybert", docs)

        generator = TopicGenerator(
            llm_client=self.client,
            model=self.llm_model,
        )

        clusters = build_cooccurence_network(keywords)

        root_topics = generator.generate_seed_topics(clusters)

        self.create_topic_embeddings(root_topics.topics)

        return root_topics

    def create_topic_embeddings(self, topics: list[Topic]):
        for topic in topics:
            self.topic_embeddings[topic.name] = self.embedder.encode(
                f"{topic.name}; {','.join(topic.keywords)}",
                prompt=_TOPIC_PROMPT.format(topic_name=topic.name),
            )

            # Recursively process subtopics if they exist
            if topic.subtopics:
                self.create_topic_embeddings(topic.subtopics)

    def classify_doc(
        self, docs: list[str], root_topics: RootTopics
    ) -> list[list[Topic]]:
        doc_embeddings = self.load_documents(docs)

        selected_topics_for_all_docs = []
        for i, emb in enumerate(doc_embeddings):
            self.logger.info("Processing document %s", docs[i])
            # Use get_all_relevant_topics instead of bfs
            relevant_topics = self.get_all_relevant_topics(root_topics, emb)
            self.logger.info(
                "Selected topics for document %s: %s",
                docs[i],
                [t.name for t in relevant_topics],
            )
            selected_topics_for_all_docs.append(relevant_topics)

        return selected_topics_for_all_docs

    def _build_ancestor_map(self, root_topics_list: list[Topic]) -> dict[str, str]:
        """Builds a map from any topic name to its ultimate root ancestor's name."""
        child_to_root_map: dict[str, str] = {}
        for root_topic in root_topics_list:
            queue = deque(root_topic.subtopics)
            visited_descendants = {root_topic}  # Avoid reprocessing, include root

            while queue:
                current_descendant = queue.popleft()
                if current_descendant in visited_descendants:
                    continue
                visited_descendants.add(current_descendant)

                child_to_root_map[current_descendant.name] = root_topic.name

                for sub_desc in current_descendant.subtopics:
                    if sub_desc not in visited_descendants:
                        queue.append(sub_desc)
        return child_to_root_map

    def group_items(self, devices: Sequence[Device]) -> list[Group]:
        docs = [f"{device.name} {device.description}".strip() for device in devices]

        root_topics = self.get_topics(docs)  # root_topics is a RootTopics object
        if not root_topics:
            self.logger.warning(
                "No topics were generated. Returning empty list of groups."
            )
            return []

        # Build the map from child topic names to their ultimate root ancestor names
        child_to_root_ancestor_map = self._build_ancestor_map(root_topics.topics)

        topic_lists = self.classify_doc(docs, root_topics)

        topic_dict: dict[Topic, list] = defaultdict(list)

        for i, (_, topics) in enumerate(
            zip(docs, topic_lists)
        ):  # topics here is a list of Topic objects
            if topics:
                for t in topics:  # t is a Topic object
                    topic_dict[t].append(devices[i])

        for topic, device_list in topic_dict.items():  # topic is a Topic object
            self.logger.debug(
                "Topic %s contains devices: %s",
                topic.name,
                [device.id for device in device_list],
            )

        groups = []
        for topic_obj, device_list in topic_dict.items():  # topic_obj is a Topic object
            if not topic_obj.subtopics:  # Create groups only for leaf topics
                top_level_ancestor_name = child_to_root_ancestor_map.get(topic_obj.name)
                parent_classes_set = (
                    {top_level_ancestor_name} if top_level_ancestor_name else set()
                )

                groups.append(
                    Group(
                        name=topic_obj.name,  # Corrected to use topic_obj.name
                        devices=device_list,
                        parent_classes=parent_classes_set,
                    )
                )

        self.logger.info(
            "Created groups with parents %s", [grp.parent_classes for grp in groups]
        )

        return groups

    def process_operations(
        self,
        existing_groups: Sequence[Group],
        devices_to_add: Sequence[Device],
        devices_to_update: Sequence[Device],
        devices_to_delete: Sequence[Device],
    ) -> list[Group]:
        pass

    def get_topic_parent(self, topic: str, topic_schema: list[Topic]):
        return [
            parent_topic
            for parent_topic in topic_schema
            if topic in [subtop.name for subtop in parent_topic.subtopics]
        ]

    def get_all_relevant_topics(
        self, root_topics: RootTopics, doc_embedding: np.ndarray
    ) -> list[Topic]:
        """
        Finds all relevant topics for a given document embedding.

        1. Calculates similarity for all topics.
        2. Min-max normalizes these similarities to a 0-1 range for the current document.
        3. Selects topics with a normalized similarity > 0.9.
        """
        all_topic_sim_scores: list[tuple[Topic, float]] = []

        # Step 1: Calculate all similarities
        queue = deque(root_topics.topics)
        visited_for_calc = set()

        while queue:
            topic = queue.popleft()
            if topic in visited_for_calc:
                continue
            visited_for_calc.add(topic)

            topic_embedding_arr = self.topic_embeddings.get(topic.name)
            if topic_embedding_arr is None:
                self.logger.warning(
                    f"No embedding for topic {topic.name} in get_all_relevant_topics"
                )
                continue

            current_doc_embedding_2d = np.atleast_2d(doc_embedding)
            topic_embedding_2d = np.atleast_2d(topic_embedding_arr)

            similarity_matrix = self.embedder.similarity(
                current_doc_embedding_2d, topic_embedding_2d
            )
            raw_similarity = similarity_matrix[0][0]
            self.logger.debug(
                f"Raw similarity for topic '{topic.name}': {raw_similarity:.4f}"
            )
            all_topic_sim_scores.append((topic, float(raw_similarity)))

            for subtopic in topic.subtopics:
                if subtopic not in visited_for_calc:
                    queue.append(subtopic)

        if not all_topic_sim_scores:
            return []

        # Step 2: Min-Max Normalization
        raw_scores = [score for _, score in all_topic_sim_scores]
        min_raw_score = min(raw_scores)
        max_raw_score = max(raw_scores)

        normalized_topic_scores: list[tuple[Topic, float]] = []
        delta = max_raw_score - min_raw_score

        if delta == 0:
            # If all raw scores are the same
            # Let's assign 0 to all, as they are all "equally bad/ambiguous".
            normalized_value = 0
            for topic, _ in all_topic_sim_scores:
                normalized_topic_scores.append((topic, normalized_value))
                self.logger.debug(
                    f"Normalized similarity for topic '{topic.name}': {normalized_value:.4f} (delta was 0)"
                )
        else:
            for topic, raw_score in all_topic_sim_scores:
                normalized_score = (raw_score - min_raw_score) / delta
                normalized_topic_scores.append((topic, normalized_score))
                self.logger.debug(
                    f"Normalized similarity for topic '{topic.name}': {normalized_score:.4f}"
                )

        # Step 3: Filter by Normalized Threshold
        selected_topics = []
        for topic, normalized_score in normalized_topic_scores:
            if normalized_score > 0.9:
                selected_topics.append(topic)

        # Log an error if no class is assigned to document
        if selected_topics in [topic.name for topic in root_topics.topics]:
            self.logger.error(
                ("document does not have any assigned topics"),
                ("re-run generate topics"),
            )

        return selected_topics
